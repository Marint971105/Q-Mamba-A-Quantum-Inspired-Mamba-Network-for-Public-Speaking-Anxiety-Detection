dataset_name = IEMOCAP
pickle_dir_path = /home/tjc/audio/QMamba/Data
features = textual,visual,acoustic
label = emotion
batch_size = 32
num_workers = 4
device = cuda:0
input_dims = [100, 512, 100]
embed_dim = 100
output_dim = 6
num_layers = 1
sequence_model = mamba
rnn_type = rnn
bidirectional = False
output_cell_dim = 50
out_dropout_rate = 0.1
network_type = QMamba
epochs = 50
lr = 0.0005
min_lr = 1e-05
unitary_lr = 0.001
weight_decay = 0.0001
patience = 10
clip = 1.0
emotion_dic = ['0', '1', '2', '3', '4', '5']
dialogue_context = False
embedding_enabled = False
loss_weights = tensor([11.5278,  6.9249,  4.3882,  6.2272,  7.8302,  3.9578], device='cuda:0')
feature_indexes = [0, 1, 2]
train_sample_num = 90
max_seq_len = 110
speaker_num = 2
train_loader = <torch.utils.data.dataloader.DataLoader object at 0x7c25dabf4ac0>
dev_loader = <torch.utils.data.dataloader.DataLoader object at 0x7c25dabf4d60>
test_loader = <torch.utils.data.dataloader.DataLoader object at 0x7c25dabf4eb0>
best_model_file = tmp/742692348
dir_name = QMamba_20250120_042109_ACC=0.5052_MacroF1=0.4990_WeightedF1=0.5058
