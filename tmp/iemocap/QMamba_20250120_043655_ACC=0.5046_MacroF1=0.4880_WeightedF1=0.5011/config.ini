dataset_name = IEMOCAP
pickle_dir_path = /home/tjc/audio/QMamba/Data
features = textual,visual,acoustic
label = emotion
batch_size = 32
num_workers = 4
device = cuda:0
input_dims = [100, 512, 100]
embed_dim = 50
output_dim = 6
num_layers = 1
sequence_model = mamba
rnn_type = rnn
bidirectional = False
output_cell_dim = 50
out_dropout_rate = 0.1
network_type = QMamba
epochs = 100
lr = 0.0005
min_lr = 1e-05
unitary_lr = 0.001
weight_decay = 0.001
patience = 15
clip = 1.0
emotion_dic = ['0', '1', '2', '3', '4', '5']
dialogue_context = False
embedding_enabled = False
loss_weights = tensor([1., 1., 1., 1., 1., 1.], device='cuda:0')
dropout = 0.3
label_smoothing = 0.1
feature_indexes = [0, 1, 2]
train_sample_num = 90
max_seq_len = 110
speaker_num = 2
train_loader = <torch.utils.data.dataloader.DataLoader object at 0x7371cc5e8b50>
dev_loader = <torch.utils.data.dataloader.DataLoader object at 0x7371cc5e8df0>
test_loader = <torch.utils.data.dataloader.DataLoader object at 0x7371cc5e8f40>
best_model_file = tmp/1423638637
dir_name = QMamba_20250120_043655_ACC=0.5046_MacroF1=0.4880_WeightedF1=0.5011
