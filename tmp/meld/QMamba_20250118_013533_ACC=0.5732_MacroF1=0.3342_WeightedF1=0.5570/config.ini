dataset_name = MELD
pickle_dir_path = /home/tjc/audio/QMamba/Data
features = textual,visual,acoustic
label = emotion
batch_size = 32
num_workers = 4
device = cuda:0
input_dims = [600, 300, 300]
embed_dim = 50
output_dim = 7
num_layers = 2
sequence_model = mamba
output_cell_dim = 50
out_dropout_rate = 0.2
network_type = QMamba
epochs = 50
lr = 0.0005
min_lr = 1e-05
unitary_lr = 0.001
weight_decay = 0.0001
patience = 10
clip = 1.0
loss_weights = tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:0')
emotion_dic = ['0', '1', '2', '3', '4', '5', '6']
dialogue_context = False
embedding_enabled = False
feature_indexes = [0, 1, 2]
train_sample_num = 1038
max_seq_len = 33
speaker_num = 9
train_loader = <torch.utils.data.dataloader.DataLoader object at 0x75c74d304c10>
dev_loader = <torch.utils.data.dataloader.DataLoader object at 0x75c74d305300>
test_loader = <torch.utils.data.dataloader.DataLoader object at 0x75c74d3054b0>
best_model_file = tmp/37013832
dir_name = QMamba_20250118_013533_ACC=0.5732_MacroF1=0.3342_WeightedF1=0.5570
