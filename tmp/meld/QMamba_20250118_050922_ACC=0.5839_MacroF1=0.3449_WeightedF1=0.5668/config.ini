dataset_name = MELD
pickle_dir_path = /home/tjc/audio/QMamba/Data
features = textual,visual,acoustic
label = emotion
batch_size = 32
num_workers = 4
device = cuda:0
input_dims = [600, 300, 300]
embed_dim = 50
output_dim = 7
num_layers = 1
sequence_model = mamba
output_cell_dim = 50
out_dropout_rate = 0.2
network_type = QMamba
epochs = 50
lr = 0.0005
min_lr = 1e-05
unitary_lr = 0.001
weight_decay = 0.0001
patience = 10
clip = 1.0
loss_weights = tensor([1.0000, 1.5000, 1.0000, 1.0000, 2.0000, 1.0000, 1.5000],
       device='cuda:0')
emotion_dic = ['0', '1', '2', '3', '4', '5', '6']
dialogue_context = False
embedding_enabled = False
feature_indexes = [0, 1, 2]
train_sample_num = 1038
max_seq_len = 33
speaker_num = 9
train_loader = <torch.utils.data.dataloader.DataLoader object at 0x7096b69f9c90>
dev_loader = <torch.utils.data.dataloader.DataLoader object at 0x7096b69fa3b0>
test_loader = <torch.utils.data.dataloader.DataLoader object at 0x7096b69fa560>
best_model_file = tmp/1300411072
dir_name = QMamba_20250118_050922_ACC=0.5839_MacroF1=0.3449_WeightedF1=0.5668
